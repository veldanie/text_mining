{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 1\n",
    "## Text Mining for Social Sciences\n",
    "## Ivan Vallejo Vall, Daniel Velasquez Vergara, Saurav Poudel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk as nl\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "#Import state-of-the-union speech\n",
    "text_raw = pd.read_csv('./speech_data_extend.txt', sep='\\t')\n",
    "\n",
    "\n",
    "#Consider paragraphs after 2000.\n",
    "text_data = text_raw.loc[text_raw['year']>=2000, :]\n",
    "\n",
    "#Number of paragraphs:\n",
    "print(len(text_data))\n",
    "\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "##1. Preprocessing of the data\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "docs = pd.Series(np.zeros(text_data.shape[0]))\n",
    "tokens = [] #List of all words.\n",
    "# Download corpora if necessary: nl.download()\n",
    "\n",
    "\n",
    "for i, line in enumerate(text_data['speech']):\n",
    "    #Tokenize the data:\n",
    "    doc_i = word_tokenize(line.lower())\n",
    "    #Remove non-alphabetic characters:\n",
    "    doc_i = [tok for tok in doc_i if tok.isalpha()]\n",
    "    #Remove stopwords using a list of your choice:\n",
    "    doc_i = [tok for tok in doc_i if tok not in stop_words]\n",
    "    #Stem the data using the Porter stemmer:\n",
    "    doc_i = [st.stem(tok) for tok in doc_i]\n",
    "\n",
    "    tokens.extend(doc_i)\n",
    "    docs.iloc[i] = doc_i\n",
    "\n",
    "\n",
    "# Corpus-level tf-idf score for every term, and choose a cutoff below which to remove words.\n",
    "unique_words = np.unique(tokens)\n",
    "lw = len(unique_words) # Number of words\n",
    "ld = len(docs) # Number of documents\n",
    "\n",
    "\n",
    "word_count = nl.FreqDist(tokens)\n",
    "tf = {k: 1+np.log(v) for k, v in word_count.items()}\n",
    "df = {k: np.sum(list(map(lambda x: k in x, docs))) for k in word_count.keys()}\n",
    "idf = {k: np.log(ld/v) for k, v in df.items()}\n",
    "tfidf = {k : v * tf[k] for k, v in idf.items() if k in tf}\n",
    "\n",
    "\n",
    "# Based on the ranking we select 500 words with highest tf-idf\n",
    "# 1st we get the rank\n",
    "import operator\n",
    "rank = sorted(tfidf.items(), key=operator.itemgetter(1), reverse=True)\n",
    "cutoff = rank[500][1]\n",
    "# 2nd apply the cut-off\n",
    "selected_words = {k: v for k, v in tfidf.items() if v>cutoff}\n",
    "ls = len(selected_words) # number of selected words: 500\n",
    "\n",
    "#Document-term matrix using 500 words selected using the tf-idf score.\n",
    "X = pd.DataFrame(np.zeros(shape = (ld, ls)), columns = selected_words.keys())\n",
    "\n",
    "for w in selected_words.keys():\n",
    "    X[w] = list(map(lambda x: x.count(w), docs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**.\n",
    "\n",
    "The data processing performed at exercise 1 allowed us to generated the document-term matrix $X$ by counting the number times that each term repeats in each document. In our analysis, a document corresponds to a paragraph of the corpus, and we considered the period from 2000 to 2014. Within this period we observed 1232 paragraphs. After computing the corpus-level tf-idf score for every term, we selected 500 words. Therefore $X$ is a $1232\\times 500$ matrix. \n",
    "\n",
    "The following script shows the process  to estimate the tf-idf-weighted document-term matrix $S$. This matrix is built based on the document-term matrix $X$. We perform the singular value decomposition on $S$, and retain 200 sigular values, i.e. $\\Sigma_{ii}=0$ for $i>200$. Then we estimate $\\hat{S}= A\\Sigma B$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##3. Generate the tf-idf-weighted document-term matrix S. Perform SVD.\n",
    "\n",
    "tf = X.copy()\n",
    "tf[tf > 0] = 1+ np.log(X[X>0]) #term frequency for each word and each document.\n",
    "\n",
    "S = X.copy()\n",
    "for i in range(ls):\n",
    "    S[tf.columns[i]] = tf.iloc[:,i] * idf[tf.columns[i]] #tf*inverse document frequency\n",
    "\n",
    "# Singular Value Decomposition:\n",
    "S_svd = np.linalg.svd(S, full_matrices=1, compute_uv=1)\n",
    "#X = A SIGMA B\n",
    "A = S_svd[0]\n",
    "SIGMA = np.vstack((np.diag(S_svd[1]),np.zeros(shape = (ld-ls, ls))))\n",
    "B = S_svd[2]\n",
    "\n",
    "#We retain 200 singular values and approximate S.\n",
    "SIGMA2 = SIGMA.copy()\n",
    "for i in range(200,ls-1):\n",
    "    SIGMA2[i,i] = 0\n",
    "\n",
    "S_hat = A.dot(SIGMA2).dot(B)\n",
    "\n",
    "\n",
    "np.sum(text_data['president']=='Obama')\n",
    "np.sum(text_data['president']=='BushII')\n",
    "# Function that estimates Cosine similarity:\n",
    "\n",
    "def cos_sim (di, dj):\n",
    "    if np.sum(di)==0 or np.sum(dj)==0:\n",
    "        sim = 0\n",
    "    else:\n",
    "        sim = np.dot(di,dj)/(np.sqrt(np.dot(di,di))*np.sqrt(np.dot(dj,dj)))\n",
    "    return (sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compare the cosine similarity of documents using both $S$ and $\\hat{S}$. As mentioned before, the period under analysis comprises 2000-2014. Within this period we have 560 documents from Obama speeches, and 544 documents from Bush speeches. For the exercise, we compute the average cosine similarity within and across speeches made by Obama and Bush. Thefore we estimata 3 averages using $S$ and another 3 averages using $\\hat{S}$. The figure below exhibits the results.\n",
    "\n",
    "![Cosine Similarity](cs.png)\n",
    "\n",
    "As expected, the average cosine similarity within Bush and within Obama, is higher than the average cosine similary when crossing documents from Bush and Obama. This result is consistent for both $S$ and $\\hat{S}$. Additionally, we observe that the similarities estimated using $\\hat{S}$ are higher than those estimated with $S$. The following script shows a function that calculates the similarity and the procedure to estimate the averages.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that estimates Cosine similarity:\n",
    "def cos_sim (di, dj):\n",
    "    if np.sum(di)==0 or np.sum(dj)==0:\n",
    "        sim = 0\n",
    "    else:\n",
    "        sim = np.dot(di,dj)/(np.sqrt(np.dot(di,di))*np.sqrt(np.dot(dj,dj)))\n",
    "    return (sim)\n",
    "\n",
    "#Cosine similarites using S and S_hat:\n",
    "S_B = np.array(S)[text_data['president']=='BushII']\n",
    "S_O = np.array(S)[text_data['president']=='Obama']\n",
    "\n",
    "S_hat_B = S_hat[text_data['president']=='BushII']\n",
    "S_hat_O = S_hat[text_data['president']=='Obama']\n",
    "\n",
    "#Bush within Average Cosine Similarity:\n",
    "bb1 = np.mean([cos_sim(S_B[i],S_B[j]) for i in range(S_B.shape[0]) for j in range(S_B.shape[0])])\n",
    "bb2 = np.mean([cos_sim(S_hat_B[i],S_hat_B[j]) for i in range(S_hat_B.shape[0]) for j in range(S_hat_B.shape[0])])\n",
    "\n",
    "#Obama within Average Cosine Similarity:\n",
    "oo1 = np.mean([cos_sim(S_O[i],S_O[j]) for i in range(S_O.shape[0]) for j in range(S_O.shape[0])])\n",
    "oo2 = np.mean([cos_sim(S_hat_O[i],S_hat_O[j]) for i in range(S_hat_O.shape[0]) for j in range(S_hat_O.shape[0])])\n",
    "\n",
    "#Bush-Obama cross Average Cosine Similarity:\n",
    "bo1 = np.mean([cos_sim(S_B[i],S_O[j]) for i in range(S_B.shape[0]) for j in range(S_O.shape[0])])\n",
    "bo2 = np.mean([cos_sim(S_hat_B[i],S_hat_O[j]) for i in range(S_hat_B.shape[0]) for j in range(S_hat_O.shape[0])])\n",
    "\n",
    "ind = np.arange(3)  # the x locations for the groups\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, (bb1, oo1, bo1), width, color='r')\n",
    "rects2 = ax.bar(ind + width, (bb2, oo2, bo2), width, color='y')\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Average Cosine Similarity')\n",
    "ax.set_title('Average Cosine Similarity \\n within and across Bush and Obama (2000, 2014)')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(('Bush-Bush', 'Obama-Obama', 'Bush-Obama'))\n",
    "ax.legend((rects1[0], rects2[0]), ('S', 'S_hat'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('cs.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "Given the document-term matrix $X$ generated in exercise 1 by counting the number times that each term repeats in each document, we estimate a multinomial mixture model using the EM algorithm. We assume there exists two topics, i.e. $K=2$. We initialize the probababily of each topic $\\rho_{k}=1/2$, for $k=1,2$. For each topic, the vector of terms probabilities $\\beta_{k}$ is initialized by drawing a sample from a Dirichlet distribution. Given that during the data-procesing stage at exercise 1 we selected 500 words, in our aplication $\\beta_{k}$ has length 500. \n",
    "\n",
    "The EM algorithm allows us to update the estimate of $\\rho_{k}$ and $\\beta_{k}$ for $k=1,2$. At each iteration we calculate the log-likehood and verify that it increses. If the increment is less than 1, we stop the procedure. The next figure shows the log-likelihood as a function of the number of iterations.\n",
    "\n",
    "![Cosine Similarity](ll.png)\n",
    "\n",
    "\n",
    "\n",
    "The table below exhibits some of the terms with highest probability after estimating $\\beta_{k}$, for $k=1,2$ using the EM algorithm.\n",
    "\n",
    "| Topic         | Terms with highest probability|\n",
    "| ------------- |:-----------------------:|\n",
    "| 1             | border, nuclear, east, Iran, Iraq, troop, Afghanistan, regim, oil|\n",
    "| 2             | social, rate, vote, benefit, medicar, parent, coverag, retir, industri|\n",
    "\n",
    "\n",
    "Given the results, we assign the label **Geopolitics** to topic 1, and **Socio-Economic** to topic 2. It is important to mention that the EM algorithm finds a local minimum, therefore, every time we run the algorithm starting from a different point we observe some changes in the results. The following script implements the EM algorithms for the multinomial mixture model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4. Multinomial Mixture Model using EM algorithm:\n",
    "#First, we define the log-likelihood:\n",
    "def log_lik (X, B, rho):\n",
    "    ll = np.sum(list(map(lambda x: np.log(np.sum([rho_i*np.prod(B_i**x) for rho_i, B_i in zip(rho, B)])), np.array(X))))\n",
    "    return(ll)\n",
    "\n",
    "#E-M Algorithm:\n",
    "K=2 #Number of Topics\n",
    "B = np.random.dirichlet(np.ones(ls), K) #Initial Beta matrix.\n",
    "rho = np.ones(K)/K # Initial rho vector.\n",
    "max_iter = 100 # Max number of iterations\n",
    "ll = [log_lik(X,B,rho)]\n",
    "for i in range(max_iter):\n",
    "    #E-step (lecture 3, slide 19):\n",
    "    z = np.array(list(map(lambda x: [rho_i*np.prod(B_i**x) for rho_i, B_i in zip(rho, B)], np.array(X))))\n",
    "    for i in range(ld):\n",
    "        z[i] = z[i]/np.sum(z[i])\n",
    "\n",
    "    #M-step (lecture 3, slide 20):\n",
    "    rho = np.sum(z, axis = 0) / np.sum(np.sum(z, axis = 0))\n",
    "    for k in range(K):\n",
    "        B[k] = np.sum([z_i*x_i for z_i, x_i in zip(z[:,k], np.array(X))], axis = 0) / np.sum([z_i*x_i for z_i, x_i in zip(z[:,k], np.sum(np.array(X),axis = 1))])\n",
    "\n",
    "    #log-likelihood at each iteration:\n",
    "    ll.extend([log_lik(X,B,rho)])\n",
    "    delta = np.abs(ll[-2] - ll[-1])\n",
    "    if delta < 1:\n",
    "        break\n",
    "\n",
    "## Top terms per topic:\n",
    "top_terms = []\n",
    "for k in range(K):\n",
    "    new_terms = X.columns[B[k]>0.006]\n",
    "    top_terms.append(new_terms)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(ll)\n",
    "plt.xlabel('Iterations');plt.ylabel('log-likelihood')\n",
    "plt.title('log-likelihood')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('ll.png')\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
